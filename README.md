# ğŸ¡ House Price Prediction

This project is a regression analysis and machine learning pipeline designed to predict housing prices based on the Ames Housing dataset. The goal is to accurately estimate the **SalePrice** of residential homes using advanced data cleaning, feature engineering, and machine learning techniques.

---

## ğŸ“ Project Structure
```
house_price_prediction_project/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”‚ â”œâ”€â”€ train_clean.csv
â”‚ â””â”€â”€ test_clean.csv
â”‚
â”œâ”€â”€ preprocess.py # Data cleaning and advanced feature engineering
â”œâ”€â”€ train_model.py # Model training and cross-validation
â”œâ”€â”€ submission.py # Prediction and submission file creation
â”œâ”€â”€ final_xgb_model.pkl # Saved final model (XGBoost)
â”œâ”€â”€ submission.csv # Submission file for Kaggle
â”œâ”€â”€ best_model_training.py # Best model training
â”œâ”€â”€ best_params.py # Finding best parameters for best model
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ log_saleprice_distribution.png # SalePrice Distribution
â””â”€â”€README.md
```

---

## ğŸ“Š Dataset

- Source: [Kaggle - House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/)
- Train size: 1,460 entries
- Test size: 1,459 entries
- Target: `SalePrice` (log-transformed as `LogSalePrice` for modeling)

---

## ğŸ”§ Features & Engineering

The dataset was extensively cleaned and transformed, including:

- Imputation of missing values based on feature types
- One-hot encoding for categorical features
- Log transformation of `SalePrice` for skew correction
- Creation of new features:
  - `TotalSF`: Total square footage
  - `Age`: Years since built/remodeled
  - `HasBasement`, `HasGarage`, etc.
  - Neighborhood and quality indicators

---

## ğŸ§  Models Trained

- Linear Regression
- Ridge Regression
- Lasso Regression
- XGBoost Regressor âœ… *(Best performing model)*

### ğŸ“ˆ Final Model Performance

- **Best CV RMSE**: `0.1428`  
- **Kaggle Private Score**: `0.14487`  
- **Model used**: `XGBoost Regressor` with tuned hyperparameters

---

## ğŸ How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/alpyaman/house-price-prediction.git
cd house-price-prediction
```
### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run Preprocessing
```bash
python preprocess.py
```

### 4. Train The Model
```bash
python train_model.py
```
### 5. Select the Best Model and Use `best_params.py` to Find Best Parameters
```bash
pyton best_params.py
```
### 6. Train the Best Model For Submission
```bash
python best_model_training.py
```
### 7. Generate Submission File
```bash
python submission.py
```
The file `submission.csv` will be saved in the root directory.

## ğŸ§ª Future Improvements
- Implementing stacking (XGBoost + Ridge + Lasso)
- Try blending multiple model outputs
- Explore neighborhood-specific modeling
- Deploy model as a web API with Flask or FastAPI

## ğŸ“Œ Requirements
- Python 3.8+
- Pandas
- NumPy
- Scikit-learn
- XGBoost
- Matplotlib
- Seaborn
- Joblib

## ğŸ“œ License
This project is licensed under the MIT License.

## ğŸ™‹â€â™‚ï¸ Author
- *Alp Yaman*
- Data Science & ML Portfolio Project
- Feel free to connect on [Linkedln](linkedin.com/in/alp-yaman-75a901174/) or check out my other projects!
